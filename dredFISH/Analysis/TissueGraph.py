"""TissueMultiGraph Analysis module.

The module contains the three main classes required graph based tissue analysis: 
TissueMultiGraph: the key organizing class used to create and manage graphs across layers
TissueGraph: object that represent a single biospatial unit tyoe
            Each layers (cells, zones, regions) is defined using 
            spatial and feature graphs that are both part ot a single tissuegraph
Taxonomy: a container class that stores information about the taxonomical units 
          (cell types, region types).  

Note
----
In current implementation each TMG object is stored as multiple files in a single directory. 
It is the same directory that stores input files used by TMG to create it's different objects. 
Only a single TMG object can exist in each directory. 
To create two TMG objects from the same data, just copy the raw data. 

"""

import pandas as pd
import numpy as np

import logging
import os.path
import json

import pickle
import base64

import re
import igraph
import pynndescent

from scipy.sparse.csgraph import dijkstra
import scipy.sparse
from scipy.spatial import cKDTree

from multiprocessing import Pool
import warnings

from scipy import stats
import anndata

from dredFISH.Utils import basicu
from dredFISH.Utils import tmgu
from dredFISH.Utils import geomu
from dredFISH.Utils import coloru
from dredFISH.Processing.Section import *
from dredFISH.Registration.Registration import *

rng = np.random.default_rng()


# define function (not nested...) to be used in parallel code
def create_and_save_geoms(sec, xy, basepath): 
    poly_dict = geomu.calc_mask_voronoi_polygons_from_XY(xy)
    sec_geoms = dict()
    for gt in ["mask","voronoi"]:
        sec_geoms[gt] = Geom(geom_type=gt, polys = poly_dict[gt],
                section = sec, basepath = basepath)
        sec_geoms[gt].save()

    return sec_geoms

# define function (not nested...) to be used in parallel code
def create_and_save_merged_geoms(sec, polys, ids, basepath,geom_type): 
    merged_polys = geomu.merge_polygons_by_ids(polys,ids)
    merged_geom = Geom(geom_type=geom_type, polys = merged_polys,section = sec, basepath = basepath)
    merged_geom.save()

    return merged_geom

class TissueMultiGraph: 
    """Main class used to manage the creation of multi-layer graph representation of tissues. 
    
    TissueMultiGraph (TMG) acts as a factory to create objects that represents the state of different biospatial units in tissue, 
    and their relationships. Examples of biospatial units are cells, isozones, regions. 
    The main objects that TMG creates and stores are TissueGraphs, Taxonomies, and Geoms.    

    Attributes
    ----------
    Layers : list 
        This is the main spatial/feature data storage representing biospatial units (cells, isozones, and regions)
        
    Taxonomies : list 
        The taxonomical representation of the different types biospatial units can have. 
        There is a many-to-one relationship between TissueGraphs Layers and Taxonomies. 
        Multiple layers can have the same taxnomy (cells and isozones both have the same taxonomy). 
        The Taxonomies store type related information (full names, feature_type_mats, relationship between types, etc). 
    
    Geoms : list of dicts
        List (one per section) of dict that contains Geom objects that represents geometrical aspects of the TMG required for Vizualization. 
        
    inputpath : str
        The top level path generated by Processing that has all the sections in it.
    basepath : str 
        Where the TMG data is saved
        
    layers_graph : list of tuples
        Stores relatioship between layers. 
        For example, [(1,2),(1,3)] inducates that layer 2 used layer 1 (isozones are build on cells) and that layer 3 (regions) uses layer 1. 
    
    layer_taxonomy_mapping : list of tuples
        Stores relationship between TG layers and Taxonomies
        
 """
    def __init__(self, 
        basepath = None,
        input_df = None,
        redo = False,
        mem_only = False,
        keep_geoms = True,  
        ):
        """Create a TMG object
        
        There could only be a single TMG object in basepath. 
        if one already exists (and redo is False) it will be loaded. If not a new empty one will be created. 
        
        Parameters
        ----------
            input_df : pandas dataframe with info about datafiles 
                       cols are: ['animal', 'section_acq_name', 'registration', 'processing', 'dataset',
                                  'dataset_path']

            basepth : a path to underwhith we save everything
                
            redo : bool (default False)
                If the object was already created in the past, the default behavior is to just load the object. 
                This can be overruled when redo is set to True. If this is the first time a TMG object is created 
            
            mem_only: if you are not planning to save this object to file, setting mem_only to True will skip creating the folder for it

        """

        # set hidden attribute _unqS to None to reduce future calls for np.unique
        self._unqS = None

        # check if file exist in basepath
        self.basepath = basepath

        # update self if this instance of TMG is with geoms or not
        self.save_taxonomies = False

        warnings.filterwarnings('ignore', category=anndata.ImplicitModificationWarning)
        
        # create basepath if needed
        if not os.path.exists(self.basepath) and not mem_only:
            os.mkdir(self.basepath, mode = 0o775)

        logging.basicConfig(
                    filename=os.path.join(self.basepath,'tmg_log.txt'),filemode='a',
                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
                    datefmt='%Y %B %d %H:%M:%S',level=logging.INFO, force=True)
        self.log = logging.getLogger("Processing")
        self.verbose = True
        
        if not redo and os.path.exists(os.path.join(self.basepath,"TMG.json")):
            self._load()
            self.log = logging.getLogger("Processing")
            self.verbose = True
            return 
        
        if keep_geoms and os.path.exists(os.path.join(self.basepath,"TMG.json")): 
            with open(os.path.join(self.basepath,"TMG.json"),encoding="utf-8") as fh:
                _config = json.load(fh)
                self.geom_to_layer_type_mapping = _config["geom_to_layer_type_mapping"]
                self.layer_to_geom_type_mapping = _config["layer_to_geom_type_mapping"]
        else: 
            # conf dict to map geoms to layer types
            self.geom_to_layer_type_mapping = { 'voronoi' : 'cell'}
            self.layer_to_geom_type_mapping = {'cell' : 'voronoi'}

        # create a new TMG object from inputs
        self.input_df = input_df
        if mem_only: 
            redo = True

        self.TMG_ver = 1
        self.Layers = list() # a list of TissueGraphs
        self.layers_graph = list() # a list of tuples that keep track of the relationship between different layers 
        self.Taxonomies = list() # a list of Taxonomies
        self.layer_taxonomy_mapping = dict() # dict() # a dictopnary that keep tracks of which TissueGraph (index into Layer) 
                                                # uses which taxonomy (index into Taxonomies)
        # conf dict to map geoms to layer types
        self.geom_to_layer_type_mapping = { 'voronoi' : 'cell'}
        
        self.layer_to_geom_type_mapping = {'cell' : 'voronoi'}

        self.adata_mapping = {"Type": "Type", #obs
                              "node_size": "node_size", #obs
                              "name" : "label", #obs
                              "XY" : "XY", #obsm
                              "Section" : "Slice"} #obs

             
        return 

    def update_user(self,message,level=20,verbose=None):
        """
        update_user Wrapper to fileu.update_user

        :param message: Message to be logged
        :type message: str
        :param level: priority 0-50, defaults to 20
        :type level: int, optional
        """
        if (self.verbose)|(verbose==True):
            print(datetime.now().strftime("%Y %B %d %H:%M:%S") + ' ' + message)
        fileu.update_user(message,level=level,logger=self.log)

    def _load(self):
        with open(os.path.join(self.basepath,"TMG.json"),encoding="utf-8") as fh:
            self._config = json.load(fh)
        self.input_df = pd.DataFrame(self._config["input_dfs"])
        # Load Taxonomies: 
        TaxNameList = self._config["tax_types"]
        Tax_path_list = self._config["tax_paths"]
        self.Taxonomies = [None]*len(TaxNameList)
        for i in range(len(TaxNameList)): 
            self.Taxonomies[i] = Taxonomy(TaxNameList[i],basepath = Tax_path_list[i])
            self.Taxonomies[i].load()

        # Load layers
        # convert string key to int key (fixing an artifects of JSON dump and load)
        ltm = self._config["layer_taxonomy_mapping"]
        ltm = {int(layer_ix): tax_ix for layer_ix, tax_ix in ltm.items()}
        self.layer_taxonomy_mapping = ltm 
        
        LayerNameList = self._config["layer_types"]
        self.Layers = [None]*len(LayerNameList)
        for i in range(len(LayerNameList)): 
            self.Layers[i] = TissueGraph(basepath = self.basepath,
                                            layer_type = LayerNameList[i], 
                                            redo = False)
            
        self.layers_graph = self._config["layers_graph"]
        self.geom_to_layer_type_mapping = self._config["geom_to_layer_type_mapping"]
        self.layer_to_geom_type_mapping = self._config["layer_to_geom_type_mapping"]

    def save(self):
        """ create the TMG.json and save everything.
        
        Saving simply iterates over all three types of objects (Layers, Taxonomies, Geom and call their respective save)
        Mapping between layers and layers/taxonomies are saved in a simple TMG.json file. 
        
        """
        # make sure layers_graph is all int
        for i,lg in enumerate(self.layers_graph): 
            self.layers_graph[i]=(int(lg[0]),int(lg[1]))
        input_df_dict = self.input_df.to_dict('list')
        self._config = { "layers_graph" : self.layers_graph, 
                         "layer_taxonomy_mapping" : self.layer_taxonomy_mapping, 
                         "layer_to_geom_type_mapping" : self.layer_to_geom_type_mapping,
                         "geom_to_layer_type_mapping" : self.geom_to_layer_type_mapping,
                         "tax_types" : [tx.name for tx in self.Taxonomies],
                         "tax_paths" : [tx.basepath for tx in self.Taxonomies],
                         "layer_types" : [tg.layer_type for tg in self.Layers],
                         "input_dfs" : input_df_dict}
        
        
        with open(os.path.join(self.basepath, "TMG.json"), 'w',encoding="utf-8") as json_file:
            json.dump(self._config, json_file)
           
        for i in range(len(self.Layers)): 
            if i == 0:
                _adata = self.Layers[0].adata
                _adata.obsm['XY'] = _adata.obsm['XY'].astype(np.float32)
            self.Layers[i].save()
        
        if self.save_taxonomies: 
            for Tx in self.Taxonomies: 
                Tx.save()
              
        self.update_user("saved")
        
    def load_geoms(self,sections = None, geom_types = None):
        # get section names 
        if sections is None: 
            sections = self.unqS
        elif isinstance(sections,str):
            sections = [sections]

        if not hasattr(self, 'Geoms') or self.Geoms is None:
            self.Geoms = [None] * len(self.unqS)

        if not isinstance(self.Geoms, list) or len(self.Geoms) != len(self.unqS):
            raise ValueError("self.Geoms must be a list with length equal to the number of unique sections (self.unqS)")

        # if geom_types not provided, load all (determined by drive)
        if geom_types is None:
            geom_path = os.path.join(self.basepath,'Geom',self.unqS[0])
            wkt_files = [f for f in os.listdir(geom_path) if f.endswith('.wkt')]
            geom_types = [file[:-4] for file in wkt_files]  # Remove the '.wkt' extension from each filename

        if isinstance(geom_types,str): 
            geom_types = [geom_types]

        for s in sections: 
            ix = self.unqS.index(s)
            section_geoms = dict()
            for gt in geom_types:
                section_geoms[gt] = Geom(geom_type=gt,polys=None,basepath=self.basepath, section=s)
            self.Geoms[ix] = section_geoms


    def get_current_type(self,layer_id):
        if isinstance(layer_id,str): 
            all_layer_types = [L.layer_type for L in self.Layers]
            try:
                layer_id = all_layer_types.index(layer_id)
            except ValueError:
                raise ValueError(f"Layer '{layer_id}' not found in available Layers.")
        curr_type = self.tax_names[self.layer_taxonomy_mapping[layer_id]]
        return curr_type

    def get_layer_type_vec(self,layer_id,tax_id = None):
        if isinstance(layer_id,str): 
            all_layer_types = [L.layer_type for L in self.Layers]
            try:
                layer_id = all_layer_types.index(layer_id)
            except ValueError:
                raise ValueError(f"Layer '{layer_id}' not found in available Layers.")
        type_ixs = self.Layers[layer_id].Type.astype(int)
        if tax_id is None:
            tax_id = self.layer_taxonomy_mapping[layer_id]
        Tax = self.get_tax(tax_id)
        type_vec = Tax.Type[type_ixs]
        return type_vec
    
    def get_layer_upstream_type_vec(self,layer_id, tax_id = None): 
        if isinstance(layer_id,str): 
            all_layer_types = [L.layer_type for L in self.Layers]
            try:
                layer_id = all_layer_types.index(layer_id)
            except ValueError:
                raise ValueError(f"Layer '{layer_id}' not found in available Layers.")
        if tax_id is None:
            tax_id = self.layer_taxonomy_mapping[layer_id]
        Tax = self.get_tax(tax_id)
        upstream_tax = Tax.upstream_tax
        if upstream_tax is None: 
            return None
        upstream_tax = self.get_tax(upstream_tax)
        
        type_vec = self.get_layer_type_vec(layer_id)
        unq_types = self.Taxonomies[tax_id].Types
        upstream_types = upstream_tax.Types

        # Create a mapping between unique types and their corresponding upstream types
        unq_type_to_upstream_type_map = dict(zip(unq_types, upstream_types))
        # convert
        upstream_type_vec = [unq_type_to_upstream_type_map[typ] for typ in type_vec]
        return upstream_type_vec


    def update_current_type(self,layer_id,tax_id): 
        # if layer_id or tax_id are not numeric, find which id they are
        if isinstance(layer_id,str): 
            all_layer_types = [L.layer_type for L in self.Layers]
            try:
                layer_id = all_layer_types.index(layer_id)
            except ValueError:
                raise ValueError(f"Layer '{layer_id}' not found in available Layers.")
        if isinstance(tax_id,str): 
            all_tax_names = [tx.name for tx in self.Taxonomies]
            try:
                tax_id = all_tax_names.index(tax_id)
            except ValueError:
                raise ValueError(f"Taxonomy '{tax_id}' not found in available taxonomies.")
            
        self.layer_taxonomy_mapping[layer_id] = tax_id
        self.Layers[layer_id].adata_mapping["Type"] = f"{self.Taxonomies[tax_id].name}_id"
    
    def add_type_information(self, layer_id, type_vec, tax): 
        """Adds type information to TMG
        
        Bookeeping method to add type information and update taxonomies etc. 
        
        Parameters
        ----------
        layer_id : int 
            what layers are we adding type info to? should be a `cell` layer 
        type_vec : numpy 1D array / list
            the integer codes of type 
        tax : int / str(Taxnomy.name) / Taxonomy
            either an integer that will be intepreted as an index to existing taxonomies or a Taxonomy object
        
        """
        if len(self.Layers) < layer_id or layer_id is None or layer_id < 0: 
            raise ValueError(f"requested layer id: {layer_id} doesn't exist")

        # if Tax is a new Taxonomy check if name exist, and if not add. 
        if isinstance(tax, Taxonomy): # add to the pool and use an index to represent it
            if tax.name in self.tax_names: 
                tax_id = self.tax_names.index(tax.name)
                self.Taxonomies[tax_id] = tax
            else: 
                self.Taxonomies.append(tax)
                tax_id = len(self.Taxonomies)-1

        if isinstance(tax,str):
            if tax in self.tax_names:
                tax = self.get_tax(tax)
            else: 
                raise ValueError(f"taxonomy {tax} not found in {self.tax_names}")

        if any(isinstance(item, str) for item in type_vec):
            type_vec = tax.get_type_ix(type_vec)
            
        # update Layer Type
        self.update_current_type(layer_id,tax_id)  
        self.Layers[layer_id].Type = type_vec

        return 
    
    def create_cell_layer(self,  
                          norm='scalar', 
                          register_to_ccf = True,
                          metric='cosine',
                          build_spatial_graph = False,
                          build_feature_graph = False,bad_bits=[],ccf_x_min=0,ccf_x_max=16):
         
        """Creating cell layer from raw data. 
        
        Loads cell data for given section using their acq name (WellX_SectionY) 
        Cell layer is unique as it's the only one where spatial information is directly used with Voronoi
        
        Parameters
        ----------

         
        """
        # allowed_options = ['logrowmedian','log','none','logregress','robust_regression']
        # if norm not in allowed_options:
        #     raise ValueError(f"Choose norm param from {allowed_options}")
        
        for TG in self.Layers:
            if TG.layer_type == "cell":
                self.update_user("!!`cell` layer already exists; return...")
                return
        # find list of sections
        adatas = []
        shared_bits = ''
        used_names = []
        self.update_user(f"Attempting To Load {self.input_df.shape[0]} Sections")
        for index,row in tqdm(self.input_df.iterrows(),desc='Loading Sections',total=self.input_df.shape[0]):
            animal = row['animal']
            section_acq_name = row['section_acq_name']
            dataset = row['dataset']
            registration_path = row['registration_path']
            processing = row['processing']
            dataset_path = row['dataset_path']
            if not os.path.exists(os.path.join(dataset_path,dataset,processing,section_acq_name)):
                self.update_user(f" Processing path Not Found {section_acq_name} {os.path.join(dataset_path,dataset,processing,section_acq_name)}")
                continue
            if not os.path.exists(os.path.join(registration_path,section_acq_name)):
                self.update_user(f" Registration path Not Found {section_acq_name} {os.path.join(registration_path,section_acq_name)}")
                continue
            try:
                adata = fileu.load(os.path.join(dataset_path,dataset,processing,section_acq_name),file_type='anndata')
            except:
                self.update_user(f"Unable to Load Data {section_acq_name}")
                continue
            if adata.shape[0]<100:
                continue
            adata.obs['animal'] = row['animal']
            adata.obs['dataset'] = row['dataset']
            adata.obs['processing'] = row['processing']
            if register_to_ccf: 
                try:
                    XYZC  = Registration_Class(adata.copy(),registration_path,section_acq_name,verbose=False,regularize=True).run()
                    adata.obs['ccf_x'] = XYZC['ccf_x']
                    adata.obs['ccf_y'] = XYZC['ccf_y']
                    adata.obs['ccf_z'] = XYZC['ccf_z']
                    """ Rename Section """
                    if adata.obs['ccf_x'].mean() < ccf_x_min:
                        self.update_user(f"Outside ccf window {section_acq_name} {adata.obs['ccf_x'].mean()}")
                        continue
                    elif adata.obs['ccf_x'].mean() > ccf_x_max:
                        self.update_user(f"Outside ccf window {section_acq_name} {adata.obs['ccf_x'].mean()}")
                        continue
                    section_name = f"{animal}_{adata.obs['ccf_x'].mean():.1f}"
                    adata.obs['old_section_name'] = section_acq_name
                    adata.obs['registration_path'] = row['registration_path']
                except:
                    self.update_user(f"Unable to Register Data {section_acq_name}")
                    continue
            else: 
                section_name = section_acq_name
            i = 1
            base_section_name =section_name
            while section_name in used_names:
                section_name = f"{base_section_name}_{i}"
                i += 1
            adata.obs['section_name'] = section_name
            adata.obs[self.adata_mapping['Section']] = section_name
            if isinstance(shared_bits,str):
                shared_bits = list(adata.var.index)
            else:
                shared_bits = [i for i in shared_bits if i in list(adata.var.index)]
            if register_to_ccf: 
                XY = np.array(adata.obs[["ccf_z","ccf_y"]])
            else: 
                XY = np.array(adata.obs[["stage_x","stage_y"]])
            self.update_user(f"Found {adata.shape[0]} cells for {section_acq_name}")
            adata = adata[:,np.isin(adata.var.index,bad_bits,invert=True)].copy()

            adata.obs['in_large_comp'] = geomu.in_graph_large_connected_components(XY,Section = None,max_dist = 0.05,large_comp_def = 0.1,plot_comp = False)
            adata = adata[adata.obs['in_large_comp']==True].copy()
            self.update_user(f"Keeping {adata.shape[0]} cells after component filtering for {section_acq_name}")

            if adata.shape[0]<100:
                self.update_user(f" Not Enough Cells Found {adata.shape[0]} cells")
                self.update_user(f"Look Into Section {section_acq_name}")
                continue

            n_cells_pre_nuc_filtering = adata.shape[0]
            adata.layers['nuc_mask'] = basicu.filter_cells_nuc(adata)
            adata = adata[np.sum(adata.layers['nuc_mask']==False,axis=1)<2].copy()
            adata = adata[np.clip(np.array(adata.layers['raw']).copy().sum(1),1,None)>100].copy()

            if adata.shape[0]<100:
                self.update_user(f" Not Enough Cells Found {adata.shape[0]} cells")
                self.update_user(f"Look Into Section {section_acq_name}")
                continue

            self.update_user(f"Keeping {adata.shape[0]} cells after nuc filtering for {section_acq_name}")
            n_cells_post_nuc_filtering = adata.shape[0]
            if n_cells_post_nuc_filtering < 0.6 * n_cells_pre_nuc_filtering:
                self.update_user(f"Likely Gel issues: More than 40% cells Removed {adata.shape[0]} cells")
                self.update_user(f"Look Into Section {section_acq_name}")
                # continue
            if adata.shape[0]<60000:
                self.update_user(f" Not Enough Cells Found {adata.shape[0]} cells")
                self.update_user(f"Look Into Section {section_acq_name}")
                # continue

            adata.X = adata.layers['raw'].copy()

            if adata.shape[0]<100:
                self.update_user(f" Not Enough Cells Found {adata.shape[0]} cells")
                self.update_user(f"Look Into Section {section_acq_name}")
                continue
            
            adata.X = basicu.normalize_fishdata_robust_regression(adata.X.copy())
            # adata.X = adata.X.sum(1).mean()*adata.X/adata.X.sum(1)[:,None]

            
            adata.X = basicu.image_coordinate_correction(adata.X.copy(),np.array(adata.obs[["image_x","image_y"]]))

            # Problematic causes over correction
            # if register_to_ccf: 
            #     adata.X = basicu.correct_linear_staining_patterns(adata.X.copy(),np.array(adata.obs[["ccf_z","ccf_y"]]))
            # else:
            #     adata.X = basicu.correct_linear_staining_patterns(adata.X.copy(),np.array(adata.obs[["stage_x","stage_y"]]))

            adata.layers['normalized'] = adata.X.copy()

            adata.layers['classification_space'] = basicu.robust_zscore(adata.X.copy())
            
            used_names.append(section_name)
            adatas.append(adata)

        adata = anndata.concat([temp[:,np.isin(temp.var.index,shared_bits)] for temp in adatas])
        self.update_user(f"{adata.shape[0]} cells across {adata.obs[self.adata_mapping['Section']].unique().shape[0]} sections")

        """ Sort adata by section name """
        adata = adata[adata.obs[self.adata_mapping['Section']].argsort()].copy()

        if register_to_ccf: 
            XY = np.array(adata.obs[["ccf_z","ccf_y"]])
        else: 
            XY = np.array(adata.obs[["stage_x","stage_y"]])
        S = np.array(adata.obs[self.adata_mapping['Section']])

        # """ Filter Out Non Cells Spatially""" #Parameterize
        # self.update_user('Filtering Cells By Spatial Proximity')
        # M = np.ones(adata.shape[0])==1
        # for section in np.unique(S):
        #     m = S==section
        #     M[m] = geomu.in_graph_large_connected_components(XY[m,:],Section = None,max_dist = 0.05,large_comp_def = 0.1,plot_comp = False)
        # adata.obs['in_large_comp'] = M#geomu.in_graph_large_connected_components(XY,Section = S,max_dist = 0.05,large_comp_def = 0,plot_comp = False)
        # adata = adata[adata.obs['in_large_comp']==True].copy()
        # self.update_user(f"{adata.shape[0]} cells across {adata.obs[self.adata_mapping['Section']].unique().shape[0]} sections")

        # """ Filter Out Non Cells By Nuc Stain""" #Parameterize
        # self.update_user('Filtering Cells By Nuc Stain')
        # adata.layers['nuc_mask'] = basicu.filter_cells_nuc(adata)
        # adata = adata[np.sum(adata.layers['nuc_mask']==False,axis=1)>0].copy() # Harshest possible filter get rid of any cells that are bad in any bit
        # self.update_user(f"{adata.shape[0]} cells across {adata.obs[self.adata_mapping['Section']].unique().shape[0]} sections")

        # """ Minimum Sum Filter """ #Parameterize
        # self.update_user('Filtering Cells By Minimum Raw Sum')
        # adata = adata[np.clip(np.array(adata.layers['raw']).copy().sum(1),1,None)>100].copy()
        # self.update_user(f"{adata.shape[0]} cells across {adata.obs[self.adata_mapping['Section']].unique().shape[0]} sections")

        if register_to_ccf: 
            XY = np.array(adata.obs[["ccf_z","ccf_y"]])
        else: 
            XY = np.array(adata.obs[["stage_x","stage_y"]])
        S = np.array(adata.obs[self.adata_mapping['Section']])

        FISHbasis = np.array(adata.layers['normalized'].copy()).copy()
        self.update_user(f"Normalizing using {norm}")
        if norm == 'robust_regression':
            FISHbasis_norm = basicu.normalize_fishdata_robust_regression(FISHbasis)
        elif norm == 'logrowmedian':
            FISHbasis_norm = basicu.normalize_fishdata_logrowmedian(FISHbasis)
        elif norm == 'log':
            FISHbasis_norm = basicu.normalize_fishdata_log(FISHbasis)
        elif norm == 'logregress': 
            FISHbasis_norm = basicu.normalize_fishdata_log_regress(FISHbasis)
        elif norm == 'none':
            FISHbasis_norm = FISHbasis.copy()
        elif norm == 'scalar_then_regression':
            FISHbasis_norm = FISHbasis.copy()
            # FISHbasis_norm = basicu.correct_linear_staining_patterns(FISHbasis_norm.copy(),XY,Section=S)
            FISHbasis_norm = basicu.batch_bit_scaling(FISHbasis_norm.copy(),Section=S)
            FISHbasis_norm = basicu.normalize_fishdata_robust_regression(FISHbasis_norm.copy())
        elif norm == 'scalar':
            FISHbasis_norm = FISHbasis.copy()
            FISHbasis_norm = basicu.batch_bit_scaling(FISHbasis_norm.copy(),Section=S)
        else:
            raise ValueError(f"Norm {norm} is not a valid option")
        adata.X = FISHbasis_norm
        adata.layers['normalized'] = FISHbasis_norm.copy()

        TG = TissueGraph(adata=adata,
                         basepath = self.basepath,
                         layer_type="cell",
                         redo=True)

        # add observations and init size to 1 for all cells
        TG.node_size = np.ones((FISHbasis_norm.shape[0],))

        # add XY and section information 
        TG.XY = XY
        TG.Section = S

        # build two key graphs
        if build_spatial_graph:
            self.update_user('building spatial graphs')
            TG.build_spatial_graph()
        if build_feature_graph:
            self.update_user('building feature graphs')
            TG.build_feature_graph(FISHbasis_norm, metric=metric)
        
        # add layer
        self.Layers.append(TG)
        self.update_user('done with create_cell_layer')
        return

    def create_merged_layer(self, base_layer_id = 0, replace = False, layer_type = None, tax_name = None, tax_composition_name = None, Labels=None):
        """
        Creates an merged_layer from existing layer. The merged layer could be an "iso" layer
        using existing Type and ensuring merges of only connected components 
        Or, it could be any labeling of original graph, if Labels are provided

        replace = True (default false) allow overwriting existing layer

        """
        if layer_type is None: 
            raise ValueError("what is the name (layer_type) of the new layer?")
        
        merged_layer_id = None
        for i,TG in enumerate(self.Layers):
            if TG.layer_type == layer_type:
                merged_layer_id = i
                if not replace: 
                    raise ValueError(f"!!{layer_type}  already exists; change replace=True to overwrite")

        if tax_name is not None: 
            self.update_current_type(base_layer_id,tax_name)

        # Contract graph. 
        MergedLayer = self.Layers[base_layer_id].contract_graph(Labels=Labels)

        # if this is a composition based merge, first determine the size of feature_mat
        if tax_composition_name is not None:
            all_tax_names = [tx.name for tx in self.Taxonomies]
            composition_tax_id = all_tax_names.index(tax_composition_name)
            
        MergedLayer.layer_type = layer_type
        if merged_layer_id is None: 
            self.Layers.append(MergedLayer)
            merged_layer_id = len(self.Layers)-1  
        else: 
            self.Layers[merged_layer_id] = MergedLayer
        self.layer_taxonomy_mapping[merged_layer_id] = self.layer_taxonomy_mapping[base_layer_id]
        self.layers_graph.append((base_layer_id,merged_layer_id))

        if tax_composition_name is not None:
            self.update_current_type(base_layer_id,tax_composition_name)
            Env = self.Layers[base_layer_id].extract_environments(typevec=self.Layers[merged_layer_id].Upstream)
            self.Layers[merged_layer_id].feature_mat = Env
    
    
    def find_upstream_layer(self, layer_id):
        """
        Use the layer graph to find the layer_id's upstream layer

        """
        upstream_layer_id = None
        for layer_pair in self.layers_graph:
            if layer_pair[1] == layer_id:
                upstream_layer_id = layer_pair[0]
                break
        return upstream_layer_id
    
    def map_to_cell_level(self, lvl, VecToMap=None, return_ix=False):
        """
        Maps values to first layer of the graph, mostly used for plotting. 
        lvl is the level we want to map all the way to 

        TODO: this function might have some problems
        """
        # if VecToMap is not supplied, will use the layer Type as default thing to map to lower levels
        if VecToMap is None: # type 
            VecToMap = self.Layers[lvl].Type.astype(np.int64)
        if isinstance(VecToMap, str) and VecToMap == 'index': 
            VecToMap = np.arange(self.Layers[lvl].N)
        elif len(VecToMap) != self.Layers[lvl].N:
            raise ValueError("Number of elements in VecToMap doesn't match requested Layer size")
            
        # if needed (i.e. not already at cell level) expand indexing backwards in layers following layers_graph
        if lvl == 0:
            return VecToMap
        else:  # (lvl > 0)
            # # while lvl > 0:
            # lvl = self.find_upstream_layer(lvl)
            # ix=ix[self.Layers[lvl].Upstream

            ix = self.Layers[lvl].Upstream
            VecToMap = VecToMap[ix].flatten()
            if return_ix: 
                return (VecToMap,ix)
            else: 
                return VecToMap
  
    @property
    def N(self):
        """list : Number of cells in each layer of TMG."""
        return([L.N for L in self.Layers])
    
    def get_N(self, section=None): 
        return([L.get_N(section=section) for L in self.Layers])
    
    def get_tax(self,tax_name): 
        tax_id = self.tax_names.index(tax_name)
        return self.Taxonomies[tax_id]

    def get_layer(self,layer_type): 
        layer_id = self.layer_names.index(layer_type)
        return self.Layers[layer_id]

    @property
    def Ntypes(self):
        """list : Number of types in each layer of TMG."""
        return([L.Ntypes for L in self.Layers])

    @property
    def layer_types(self):
        """list : layer_type of each layer in TMG"""
        return([L.layer_type for L in self.Layers])
    
    def find_layer_by_name(self, layer_type):
        all_layer_types = np.array(self.layer_types)
        ix = np.flatnonzero(all_layer_types == layer_type)
        if len(ix) == 0:
            self.update_user(f"No layer of type {layer_type} was found.")
            self.update_user("Check layer_types to see what options already exist in a TMG object") 
            return(ix)
        if len(ix) != 1: 
            raise ValueError("More then one layer has the same name, please check")
        return(ix[0])


    @property
    def Nsections(self):
        """int : Number of unique sections"""
        return(len(self.unqS))

    @property
    def unqS(self):
        if self._unqS is None:
            assert len(self.Layers)
            self._unqS = self.Layers[0].unqS
        #     Sections = self.Layers[0].Section
        #     """ order based on ccf_location {animal}_{ccf_x}"""
        #     def get_ccf_x(section):
        #         return float(section.split('_')[1])
        #     # Sort Sections based on ccf_x from lowest to highest
        #     self._unqS = sorted(np.unique(Sections), key=get_ccf_x)
        #     # self._unqS = list(np.unique(Sections))
        # # return a list of (unique) sections 
        return(self._unqS)

    @property
    def tax_names(self): 
        return [tx.name for tx in self.Taxonomies]

    @property
    def layer_names(self):
        return [l.layer_type for l in self.Layers]
    
    def add_and_save_vor_mask_geoms(self):
        XY_per_section=self.Layers[0].get_XY(section=self.unqS)
        XY_per_section_dict = {s: xy for s, xy in zip(self.unqS, XY_per_section)}
        args = [(sec, XY_per_section_dict[sec],self.basepath) for sec in self.unqS]
        with multiprocessing.Pool() as pool:
            self.Geoms = pool.starmap(create_and_save_geoms, args)

    def add_and_save_merged_geoms(self,merged_layer_id):
        # make sure geom are loaded  
        if not self.Geoms:
            raise ValueError("Can't merge geoms when they are not loaded...")
        # figure out the geom type of the base layer
        base_layer_id = self.find_upstream_layer(merged_layer_id)
        base_layer_type = self.Layers[base_layer_id].layer_type
        base_layer_geom_type = self.layer_to_geom_type_mapping[base_layer_type]

        # create the args for parallel processing
        base_poly_per_sections = [gt[base_layer_geom_type].polys for gt in self.Geoms]
        merge_ids = np.array(self.Layers[merged_layer_id].Upstream)
        merged_ids_sections_dict = {s : merge_ids[self.Layers[base_layer_id].Section == s] for s in self.unqS}
        base_poly_per_sections_dict = {s: poly for s, poly in zip(self.unqS, base_poly_per_sections)}
        args = [(sec, base_poly_per_sections_dict[sec],
                merged_ids_sections_dict[sec],
                self.basepath,
                self.Layers[merged_layer_id].layer_type) for sec in self.unqS]

        # run multiprocessing merge and save
        with multiprocessing.Pool() as pool:
            geoms_list = pool.starmap(create_and_save_merged_geoms, args)
        
        merge_layer_geom_type = self.Layers[merged_layer_id].layer_type
        self.geom_to_layer_type_mapping[merge_layer_geom_type]=merge_layer_geom_type
        self.layer_to_geom_type_mapping[merge_layer_geom_type]=merge_layer_geom_type
        for i,g in enumerate(geoms_list): 
            self.Geoms[i][merge_layer_geom_type]=g


    def load_stiched_labeled_image(self,section = '',label_type = 'total',flip = True):
        if section is None:
            section = self.unqS[0]
        
        # load from drive using fileu
        lbl = fileu.load(os.path.join(self.inputpath,section),file_type='mask',model_type = label_type)
        lbl = lbl.numpy()
        # zero out any labels in the mask do not mattch the TG names
        # does that by finding layers, getting names, subsetting to section
        names = self.Layers[0].names
        Sections = self.Layers[0].Section
        ix = np.flatnonzero(Sections == section)
        names = names[ix]
        # find all unique labels in the lbl matrix
        i, j = np.nonzero(lbl)
        unq_lbl = np.unique(lbl[i,j])
        unq_lbl_flt = np.copy(unq_lbl)
        unq_names = np.unique(names)
        ix_flt = np.logical_not(np.isin(unq_lbl_flt,unq_names))
        unq_lbl_flt[ix_flt]=0
        
        # zeros out labels that are not in names
        lookup_o2n = pd.Series(unq_lbl_flt, index=unq_lbl)
        lbl_filtered = geomu.swap_mask(lbl, lookup_o2n)

        if flip:
            lbl_filtered = np.transpose(lbl_filtered) 

        return(lbl_filtered)

class TissueGraph:
    """Representation of transcriptional state of biospatial units as a graph. 
    
    TissueGraph (TG) is the core class used to analyze tissues using Graph representation. 
    TG stores a two layer graph G = {V,Es,Ef} where Es are spatial edges (physical neighbors) and Ef are feature neighnors. 
    TG stores information on position (XYS) and features that used to created these graphs using anndata object representation.  
    Each TG has a reference to a Taxonomy object that contains information on the types ( 
    
    Note
    ----
    TissueGraph objects are typically not created on their own, but using method calls of TMG (create_{cell,isozones,regions}_layer)
    
    Attributes
    ----------
        tax : Taxonomy
            a Taxonomy object that contain labels, type stats, and relationship between types. 
        SG : iGraph
            Graph representation of the spatial relationship between biospatiual units (i.e. cells, zones, regions). 
            SG might include multiple componennts for multiple section data and is non-weighted graph (1 - neighbors, 0 not neighbors). 
        FG : iGraph 
            Graph representation of feature similarity between biospatial units.  
               
        
    main methods:
        * contract_graph - find zones/region, i.e. spatially continous areas in the graph the same (cell/microenvironment) type
        * cond_entopy - calculates the conditional entropy of a graph given types (or uses defaults existing types)
        * watershed - devide into regions based on watershed


    """
    def __init__(self, basepath=None, layer_type=None,
                       feature_mat=None, feature_mat_raw=None,
                       redo=False, obs=None,layers=None,
                       adata=None
        ):
        """Create a TissueGraph object
        
        Parameters
        ----------
        feature_mat : numpy 2D arrary or tuple of matrix size
            Matrix of the spatial units features (expression, composition, etc). 
            As alternative to the full matrix, input could be a tuple of matrix size (samples x features)
        basepath : str
            Where to read/write files related to this TG
        layer_type : str
            Name of the type of layer (cells, isozones, parcellation, etc. )
        """

        # validate input
        if basepath is None: 
            raise ValueError("missing basepath in TissueGraph constructor")
        if layer_type is None: 
            raise ValueError("Missing layer type information in TissueGraph constructor")

        # what is stored in this layer (cells, zones, regions, etc)
        self.layer_type = layer_type # label layers by their type
        self.basepath = basepath

        logging.basicConfig(
                    filename=os.path.join(self.basepath,'tmg_log.txt'),filemode='a',
                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
                    datefmt='%Y %B %d %H:%M:%S',level=logging.INFO, force=True)
        self.log = logging.getLogger("Analysis")
        self.verbose = True
 
        # this dict stores the defaults field names in the anndata objects that maps to TissueGraph properties
        # this allows storing different versions (i.e. different cell type assignment) in the anndata object 
        # while still maintaining a "clean" interfact, i.e. i can still call for TG.Type and get a type vector without 
        # knowing anything about anndata. 
        # To see where in AnnData everything is stored, check comment in rows below 
        self.adata_mapping = {"Type": "Type", #obs
                              "node_size": "node_size", #obs
                              "name" : "label", #obs
                              "XY" : "XY", #obsm
                              "Z"  : "ccf_x", #obs
                              "Section" : "Slice"} #obs
        # Note: a these mapping are not used for few attributes such as SG/FG/Upstream that are "hard coded" 
        # as much as possible, the only memory footprint is in the anndata object, the exceptions are SG/FG that 
        # are large objects that we want to keep as iGraph in mem. Therefore, SG/FG are created during init from adata.obsp
        
        # Key graphs - spatial and feature based
        self.SG = None # spatial graph (created by build_spatial_graph, or load in __init__)
        self.FG = dict() # Feature graph (created by build_feature_graph, or load in __init__)
        self._spatial_edge_list = None # for performance (of .contract_graph), going to save the edge list extermally from self.SG 

        # there are two mode of TG loading: 
        # 1. standard (from drive): load the full adata and create SG and FG 
        # 2. from scratch : uses input arguments to rebuild the TG object from scratch, ignores anything in the drive. 
        
        if not redo:
            layer_path = os.path.join(self.basepath, 'Layer')
            self.adata = anndata.read_h5ad(os.path.join(layer_path,f"{self.layer_type}_layer.h5ad"))
            
            if 'adata_mapping' in self.adata.uns:
                self.adata_mapping = self.adata.uns["adata_mapping"]
            
            # build all graphs
            # SG is saved as a list of spatial graphs (one per section)
            for g_name in self.adata.obsp.keys():
                if g_name=="SG":

                    # create SG and FG from Anndata
                    sg = self.adata.obsp["SG"] # csr matrix
                    self.SG =  tmgu.adjacency_to_igraph(sg, directed=False, simplify = False)
                else: 
                    fg = self.adata.obsp[g_name] # csr matrix
                    # self.FG[g_name] = dict()
                    # self.FG[g_name]["G"] = tmgu.adjacency_to_igraph(fg, directed=False, simplify = False)

            
            if "SG_knn" in self.adata.uns:
                try: 
                    filename = os.path.join(layer_path,self.adata.uns["SG_knn"])
                    with open(filename,'rb') as file: 
                        knn = pickle.load(file)
                    self.SG_knn = knn
                except Exception:
                    self.update_user("Failed to load SG_knn - please check")
                    self.SG_knn = None

            for key in self.adata.uns.keys():
                if key.startswith("FG_") and key.endswith("_knn"):
                    fg_key = key[3:-4]  # Remove 'FG_' prefix and '_knn' suffix
                    filename = os.path.join(layer_path,self.adata.uns[key])
                    try:
                        with open(filename, 'rb') as file:
                            knn = pickle.load(file)
                        self.FG[fg_key]["knn"] = knn
                    except Exception:
                        self.update_user(f"Failed to load {key} - please check")
                        self.FG[fg_key]["knn"] = None
            
        else: # create an object from given feature_mat data

            # if feautre_mat is a tuple, replace with an empty sparse matrix
            if isinstance(feature_mat,tuple): 
                feature_mat = scipy.sparse.csr_matrix(feature_mat)

            # The main data container is an anndata, initalize with feature_mat
            if adata is not None:
                self.adata = adata.copy()
            elif feature_mat is None: 
                self.adata = anndata.AnnData(feature_mat, obs=obs)
            else: 
                self.adata = anndata.AnnData(feature_mat, obs=obs,dtype=feature_mat.dtype) # the tissuegraph AnnData object

            if feature_mat_raw is not None:
                self.adata.layers['raw'] = feature_mat_raw

        if layers!=None:
            self.adata.layers = layers

        return 

    def update_user(self,message,level=20):
        """
        update_user Wrapper to fileu.update_user

        :param message: Message to be logged
        :type message: str
        :param level: priority 0-50, defaults to 20
        :type level: int, optional
        """
        if self.verbose:
            print(datetime.now().strftime("%Y %B %d %H:%M:%S") + ' ' + message)
        fileu.update_user(message,level=level,logger=self.log)
    
    def is_empty(self):
        """Determines if the TG object is empty
        
        Checks if internal adata is None of empty
        """ 
        if self.adata is None or self.adata.shape[0]==0: 
            return True
        else: 
            return False

    def filter(self,logical_vec,rebuild_SG = True): 
        """
        removes observations from TG. 
        can only work if layer_type==cells
        """
        if self.layer_type != "cell": 
            raise ValueError("can only filter at the cell level")

        self.adata = self.adata[logical_vec]

        # rebuild igraph layers (that are only saved as adj matrix within anndata)
        if rebuild_SG and self.SG != None: 
            if self.SG_knn is None: 
                self.build_spatial_graph()
            else: 
                self.build_spatial_graph(save_knn=True)
        else:
            # after filtering, need to rebuild SG, if it existed (and rebuild_SG was False) zeros it out
            self.SG = None
            self.SG_knn = None
        
        self.FG = dict()

    def save(self):
        """ add stuff to adata"""
        self.adata.uns["adata_mapping"]=self.adata_mapping

        """save TG to file"""
        layer_path = os.path.join(self.basepath, 'Layer')
        if not os.path.exists(layer_path):
            os.makedirs(layer_path)
        if hasattr(self, 'SG_knn'):
            filename =  f"{self.layer_type}_SG_knn.pkl" # 
            fullfilename = os.path.join(layer_path,filename)
            with open(fullfilename, 'wb') as file:
                pickle.dump(self.SG_knn,file)
            self.adata.uns["SG_knn"] = filename
        for g in self.FG.keys():
            if 'knn' in self.FG[g]:
                filename = f"{self.layer_type}_{g}_knn.pkl"
                fullfilename = os.path.join(layer_path,filename)
                with open(fullfilename, 'wb') as file:
                    pickle.dump(self.FG[g]['knn'], file)
                self.adata.uns[f"{g}_knn"] = filename
        
        if not os.path.exists(layer_path):
            os.makedirs(layer_path)
        if not self.is_empty():
            self.adata.write(os.path.join(layer_path,f"{self.layer_type}_layer.h5ad"))

    
    @property
    def names(self):
        """list : observation names"""
        if self.is_empty():
            return None
        return self.adata.obs[self.adata_mapping["name"]]
    
    def get_names(self,section = None):
        if section is None: 
            return(self.names)
        else:
            return(self.names[self.Section == section])

    @names.setter
    def names(self,names):
        self.adata.obs[self.adata_mapping["name"]]=names
    
    @property
    def Upstream(self):
        """list : mapping between current TG layer (self) and upstream layer
        Return value has the length of upstream level and index values of current layer""" 
        if self.is_empty():
            return None
        # Upstream is stored as uns in adata: 
        return self.adata.uns["Upstream"]
    
    @Upstream.setter
    def Upstream(self,V):
        self.adata.uns["Upstream"]=V
    
    @property
    def spatial_edge_list(self):
        if self._spatial_edge_list is None: 
            if self.SG is None: 
                raise ValueError('cannot return edge list before spatial graph was created')
            self._spatial_edge_list = np.array(self.SG.get_edgelist(),dtype=int)

        return self._spatial_edge_list.copy()

    @property
    def feature_mat_shape(self):
        return self.adata.shape

    @property
    def feature_mat(self):
        """matrix : the feature values for this TG observations
        
        The feature_mat is stored in the underlying anndata object and is required to properly init it. 
        """
        # if adata is still None, return None
        if self.is_empty() or self.adata.X is None:
            return None
        # otherwide, feature_mat is stored as the main data in adata
        return(self.adata.X.copy())
    
    @feature_mat.setter
    def feature_mat(self,X):
        """ update feature_mat (adata.X)
        Key issue is that anndata doesn't allow changing number of cols
        if we need to, remake adata
        """
        if self.adata.shape[1] == X.shape[1]:
            self.adata.X = X
        else: 
            obs = self.adata.obs
            obsm = self.adata.obsm
            obsp = self.adata.obsp
            uns = self.adata.uns
            layers = self.adata.layers
            if isinstance(X, pd.DataFrame):
                var = X.columns
                X = X.values
                self.adata = anndata.AnnData(X=X, obs=obs, obsm=obsm, obsp=obsp,var=var,uns=uns,layers=layers)
            else: 
                self.adata = anndata.AnnData(X=X, obs=obs, obsm=obsm, obsp=obsp,uns=uns,layers=layers)
    
    def get_feature_mat(self,section = None):
        if section is None: 
            return(self.feature_mat.copy())
        else: 
            return(self.feature_mat[self.Section == section,:])

    @property 
    def Type(self): 
        """Type
        """
        if self.is_empty():
            return None
        elif self.adata_mapping["Type"] not in self.adata.obs.columns.values.tolist(): 
            return None
            # raise ValueError("Mapping of type to AnnData is broken, please check!")
        else:
            typ = self.adata.obs[self.adata_mapping["Type"]]
            typ = np.array(typ) 
            return typ
        
    @Type.setter
    def Type(self,Type):
        """list : list (or 1D np array) of integer values that reference a Taxonomy object types""" 
        self.adata.obs[self.adata_mapping["Type"]] = Type
        
    @property
    def N(self):
        """int : Size of the tissue graph
            internally stored as igraph size
        """
        if not self.is_empty():
            return(self.adata.shape[0])
        else: 
            raise ValueError('TissueGraph does not contain an AnnData object, please verify!')
    
    @property
    def node_size(self):
        if self.is_empty():
            return None
        elif self.adata_mapping["node_size"] not in self.adata.obs.columns.values.tolist(): 
            raise ValueError("Mapping of type to AnnData is broken, please check!")
        else: 
            return self.adata.obs[self.adata_mapping["node_size"]]
    
    @node_size.setter
    def node_size(self,Nsz):
        self.adata.obs[self.adata_mapping["node_size"]] = list(Nsz)
    
    @property
    def Section(self):
        """
            Section : dependent property - will query info from anndata and return
        """
        if self.adata is None:
            return None
        elif self.adata_mapping["Section"] not in self.adata.obs.columns.values.tolist(): 
            raise ValueError("Mapping of type to AnnData is broken, please check!")
        else: 
            return self.adata.obs[self.adata_mapping["Section"]]

    
    @property
    def unqS(self):
        
        def get_ccf_x(section):
            return float(re.split('[_\.]', section)[1])
        return sorted(np.unique(self.Section), key=get_ccf_x)

    @property
    def size_of_sections(self):
        # Likely Broken
        _,count = np.unique(self.Section,return_counts = True)
        return(count)

    @Section.setter
    def Section(self,Section):
        self.adata.obs[self.adata_mapping["Section"]]=Section

    @property
    def XY(self):
        """
            XY : dependent property - will query info from anndata and return
        """
        if self.adata is None:
            return None
        elif self.adata_mapping["XY"] not in self.adata.obsm.keys(): 
            raise ValueError("Mapping of XY to AnnData is broken, please check!")
        else: 
            return self.adata.obsm[self.adata_mapping["XY"]].copy()

    def get_XY(self,section = None):
        if section is None: 
            return(self.XY)
        elif isinstance(section, list):
            return [self.XY[self.Section == sec, :] for sec in section]
        else: 
            return(self.XY[self.Section == section,:])

    def get_obs(self,column,section = None):
        if section is None: 
            return(self.adata.obs[column])
        elif isinstance(section, list):
            return [self.adata.obs[column][self.Section == sec, :] for sec in section]
        else: 
            return(self.adata.obs[column][self.Section == section, :])
        
    def get_N(self, section=None): 
        if section is None: 
            return self.N
        else: 
            return np.sum(self.Section==section)

    @XY.setter
    def XY(self,XY): 
        self.adata.obsm[self.adata_mapping["XY"]]=XY
        
    @property    
    def X(self):
        return(self.XY[:,0])
        
    @property
    def Y(self):
        return(self.XY[:,1])
    
    @property
    def Z(self): 
        if self.adata is None:
            return None
        elif self.adata_mapping["Z"] not in self.adata.obs.keys(): 
            raise ValueError("Mapping of Z to AnnData is broken, please check!")
        else: 
            return self.adata.obs[self.adata_mapping["Z"]]

    @property
    def XYZ(self): 
        XY = self.XY
        Z = np.array(self.Z)
        XYZ = np.hstack((XY,Z[:,np.newaxis]))
        return XYZ

    @Z.setter
    def Z(self,Z): 
        self.adata.obs[self.adata_mapping["Z"]]=Z
    
    @property    
    def Ntypes(self): 
        """ 
            Ntypes: returns number of unique types in the graph
        """ 
        if self.Type is None: 
            raise ValueError("Type not yet assigned, can't count how many")
        return(len(np.unique(self.Type)))
    
    @property
    def Nsections(self):
        """
            Nsections : returns number of unique sections in TG
        """
        unqS = np.unique(self.Section)
        return(len(unqS))
    
    def build_feature_graph(self, X = None, n_neighbors=15, name = None,
                                  metric='cosine', accuracy={'prob':1, 'extras':1.5}, metric_kwds={}, return_graph=False,save_knn = False):
        """construct k-graph based on feature similarity

        Create a kNN graph (an igraph object) based on feature similarity. The core of this method is the calculation on how to find neighbors. 
        If metric is "precomputed" the distances are assumed to be known and we're almost done. 
        For all other metric values, we use pynndescent 

        Parameters
        ----------
        X : numpy array
            Either a distance matrix, i.e. squareform(pdist(feature_mat)) if metric = 'precomputed'.  ) or just a feature_mat
            by defaults (if it's None) will use self.feature_mat
        n_neighbors : int
            How many neighbors (k) should we use in the knn graph
        metric : str
            either "precomputed", "random", or one of the MANY metrics supported by pynndescent. Random is for debugging only. 
        accuracy : dict with fields: 'prob' and 'extras'
            a dictionary with accuracy options for pynndescent. 'prob' should be in [0,1] and 'extras' is typically >1. 
            accuracy['prob'] conrols the 'diversify_prob' and accuracy['extra'] the 'pruning_degree_multplier' 
        metric_kwds : dict
            passthrough kwds that will be sent to pynndescent. 
        return_graph : bool
            will return the graph instead of updating self.FG

        Note
        ----
        There are LOTS of metric implemnted in pynndescent. 
        Many are not updated in the readthedocs so check the sources code! 
        """
    
        self.update_user(f"building feature graph using {metric}")
        if metric is None:
            raise ValueError('metric was not specified')

        # If X is none, use feature_mat
        if X is None: 
            X = self.feature_mat

        # if name isn't provided, use metric for name
        if name is None: 
            name = metric

        (G,knn) = tmgu.build_knn_graph(X,metric,n_neighbors=n_neighbors,accuracy=accuracy,metric_kwds=metric_kwds)

        self.adata.obsp[name] = G.get_adjacency_sparse()
        # save new features graph as 
        self.FG[name] = dict()
        self.FG[name]["G"] = G
        if save_knn: 
            self.FG[name]["knn"] = knn

        if return_graph:
            return G

    def knn_query(self,X,k=15,graph = 'spatial',verbose = False):
        """
        performs knn query on feature/spatial graph. 

        Feature graph uses standard knn

        Spatial graph works "per section" so X needs to be a tuple with (XY,Section) of which 
        section would each XY point belong to. 
        """
        if graph == 'spatial': 
            closest_sections=X[1]
            XY_qry = X[0]
            indices = np.zeros((XY_qry.shape[0],k))
            distances = np.zeros((XY_qry.shape[0],k))
            offset = 0
            for i,s in enumerate(self.unqS):
                if verbose: 
                    print(f"querying neighbors for section {s} {i}/{len(self.unqS)}") 
                ix = np.flatnonzero(closest_sections==s)
                if isinstance(self.SG_knn[i], cKDTree):
                    distances[ix,:],indices[ix,:] =  self.SG_knn[i].query(XY_qry[ix,:],k)
                else: 
                    indices[ix,:],distances[ix,:] =  self.SG_knn[i].query(XY_qry[ix,:],k)
                indices[ix,:] += offset
                offset += np.sum(self.Section == s)
            return (indices,distances) 
        elif graph in self.FG:
            knn = self.FG[graph]["knn"]
            indices,distances = knn.query(X,k)
            return (indices,distances)
        else: 
            raise ValueError("Graph type must be 'spatial' of a known feature graph")

        
    
    def build_spatial_graph(self,max_dist = 300,save_knn = False):
        """construct graph based on Delaunay neighbors
        
        build_spatial_graph will create an igrah using Delaunay triangulation
        
        Spatial graph can potentially be a multi-component one. Cells cannot be neighbors if they are in different sections
        or or they are more than max_dist away from each other. 

        This methods assumes that the order of the rows in the TG.adata will 
        sorted by section. This happens naturally when using create_cell_layer with input_df of one 
        per section. If not - adjust it while you build the cell layer

        in addition to the "per section" graphs, also builds a list of knn graphs to store for future queries

        """
        unqS = self.unqS
        self.update_user(f"Building spatial graphs for {self.Nsections} sections")
        # section = self.Section.reset_index(drop=True)
        # sorted_section = section.sort_values().reset_index(drop=True)
        # if not np.all(sorted_section == section):
        #     raise ValueError("Sections are not sorted. Please sort the sections before building the spatial graph.")
        # # self.adata = self.adata[self.adata.obs[self.adata_mapping['Section']].argsort()]
        self.SG = list()
        for s in range(self.Nsections): 
            # get XY for a given section
            XY_per_section = self.XY[self.Section==unqS[s],:]
            self.SG.append(geomu.spatial_graph_from_XY(XY_per_section,max_dist=max_dist))
            
        # to merge the spatial graphs into one with many components: 
        self.SG = igraph.Graph.disjoint_union(self.SG[0],self.SG[1:])
        
        self.update_user("updating anndata")
        self.adata.obsp["SG"] = self.SG.get_adjacency_sparse()
        self.adata.obs[self.adata_mapping["node_size"]] = np.ones(self.XY.shape[0])

        # get XYZ data
        # building KNN objects in the same way that the grpah was build, per section
        if save_knn:
            self.update_user("building knn query object")
            self.SG_knn = list()
            for s in self.unqS: 
                XY_per_section = self.XY[self.Section==s,:]
                _,knn = tmgu.build_knn_graph(XY_per_section,'euclidean')
                self.SG_knn.append(knn)
        self.update_user("done building spatial graph")


    def add_spatial_graph_edges(self,min_distance=2,max_distance=3):
        """
        creates additional connection between nodes that are proximate (between min,max) and are the same type 
        """
        # get list of candidate edges to add, i.e. they are between distance min and max of each other
        EL = tmgu.find_node_pairs_by_distance(self.SG, min_distance, max_distance)        
        TypeVec = self.Type
        
        # only keep such edges that are across unconnected type components
        ELself = self.spatial_edge_list
        ELself = ELself[np.take(TypeVec,ELself[:,0]) == np.take(TypeVec,ELself[:,1]),:]
        TypeGraph = igraph.Graph(n=self.N, edges=ELself, directed = False)
        cmp = TypeGraph.components().membership
        # make sure that they are the same type

        edge_to_keep = np.logical_and(np.take(TypeVec,EL[:,0]) == np.take(TypeVec,EL[:,1]),
                              np.take(cmp,EL[:,0]) != np.take(cmp,EL[:,1]))

        EL = EL[edge_to_keep,:]

        self.SG.add_edges(EL)
        # reset spatial_edge_list so that we get it from SG next time
        self._spatial_edge_list = None

    
    def contract_graph(self, Labels = None, feature_mat_calc = 'mean',return_useful_layer = True):
        """ create a new TG that is a contraction of current one. 
        Two types of contractions are supported: 
        1. Using Type - when Labels is None will use self.Type 
                        using spatially aware, i,e, will only merge connected components that have the same type
        2. Using Labels - when Labels are provided, will just use these to merge nodes. 
                          Doesn't care if they are/aren't connected, they still get merged

        To speed calculation, can skip some steps if return_useful_layer is False

        Parameters
        ----------
        Labels : 1D numpy array with dtype int (default value is self.Type)
                 a vector of Types for each node. If None, will use self.Type

        Note
        ----
        Default behavior is to assign the contracted TG the same taxonomy as the original graph. 
        
        Returns
        -------
        TissueGraph 
            A TG object after vertices merging. 
        """

        # If Labels are non, use Type on connected component manner
        if Labels is None:
            Labels = self.Type 
            # get edge list - note that Spatial graphs work with indexes not cell names      
            EL = self.spatial_edge_list
        
            # only keep edges where neighbors are of same types
            EL = EL[np.take(Labels,EL[:,0]) == np.take(Labels,EL[:,1]),:]
        
            # remake a graph with potentially many components
            IsoZonesGraph = igraph.Graph(n=self.N, edges=EL, directed = False)

            # because we used both type and proximity, the original graph (based only on proximity)
            #  will be broken down to multiple components (for each original component)
            # finding clusters for each component. 
            cmp = IsoZonesGraph.components()
        
            IxMapping = np.asarray(cmp.membership)
        else: 
            _,IxMapping = np.unique(Labels,return_inverse=True)
        
        ZoneName, ZoneSingleIx, ZoneSize = np.unique(IxMapping, return_index=True, return_counts = True)
        
        # calculate zones feature_mat
        # if Labels where provided, , 
        # just replace with tuple of the required size
        if not return_useful_layer or feature_mat_calc is None:
            zone_feat_mat = (len(ZoneSize),1)
        elif feature_mat_calc == 'mean':
            df = pd.DataFrame(data = self.feature_mat)
            df['type']=IxMapping
            zone_feat_mat = np.array(df.groupby(['type']).mean())
        else: 
            raise ValueError(f"unknown value {feature_mat_calc} for feature_mat_calc")
            
        # create new SG for zones 
        ZSG = self.SG.copy()
        ZSG.contract_vertices(IxMapping)
        ZSG.simplify()

        # create a new Tissue graph by copying existing one, contracting, and updating XY
        ZoneGraph = TissueGraph(feature_mat=zone_feat_mat, 
                                basepath=self.basepath,
                                layer_type="TBD",
                                redo=True,
                                )
        
        # recreate contracted graph as a TissueGraph
        if return_useful_layer:
            # use weighted bincount to calc XY fast
            newX = np.bincount(IxMapping,weights = self.XY[:,0]) / ZoneSize
            newY = np.bincount(IxMapping,weights = self.XY[:,1]) / ZoneSize
            newZ = np.bincount(IxMapping,weights = self.Z) / ZoneSize
            new_XY = np.zeros((len(newX),2))
            new_XY[:,0] = newX
            new_XY[:,1] = newY
        
            # assigne seciton name using unique indexes
            _,unq_ix = np.unique(IxMapping, return_index = True)
            new_section = list(self.Section[unq_ix])

            ZoneGraph.XY = new_XY
            ZoneGraph.Z = newZ
            ZoneGraph.Section = new_section
            

        ZoneGraph.SG = ZSG
        ZoneGraph.names = ZoneName
        ZoneGraph.node_size = ZoneSize
        ZoneGraph.Type = Labels[ZoneSingleIx]
        ZoneGraph.Upstream = IxMapping
        
        return(ZoneGraph)
                             
    def type_freq(self, max_val = None, section = None): 
        """return the catogorical probability for each type in TG
        
        Probabilities are weighted by the node_size
        """
        if self.Type is None: 
            raise ValueError("Type not yet assigned, can't count frequencies")
        
        if max_val is None: 
            max_val = np.max(self.Type)

        if section is not None: 
            ix = self.Section==section
        else: 
            ix = np.ones(self.N,dtype=bool)
        
        cnts = np.bincount(self.Type[ix], weights=self.node_size[ix], minlength=max_val+1) 
        Ptypes = cnts / np.sum(cnts) 

        
        return Ptypes
    
    def cond_entropy(self,return_all = False):
        """calculate conditional entropy of the tissue graph
           
           cond entropy is the difference between graph entropy based on zones and type entropy
        """
        Pzones = self.node_size
        Pzones = Pzones/np.sum(Pzones)
        Entropy_Zone = -np.sum(Pzones*np.log2(Pzones))
        
        # validate that type exists
        if self.Type is None: 
            raise ValueError("Can't calculate cond-entropy without Types, please check")
            
        Ptypes = self.type_freq()
        Ptypes = Ptypes[Ptypes>0]
        Entropy_Types=-np.sum(Ptypes*np.log2(Ptypes))
        
        cond_entropy = Entropy_Zone-Entropy_Types
        if return_all: 
            return (Entropy_Zone,Entropy_Types,cond_entropy)
        else: 
            return(cond_entropy)
    
    def extract_environments(self,ordr = None,typevec = None, k=None, N = None, weights = None):
        """returns the categorical distribution of neighbors. 
        
        Depending on input there could be two uses, 
            usage 1: if ordr is not None returns local neighberhood defined as nodes up to distance ordr on the graph for all vertices. 
            usage 2: if typevec is not None returns local env based on typevec, will return one env for each unique type in typevec
            usage 3: if k is not None, calcualte knn spatially per section and then calculate env based on that. 
        Return
        ------
        numpy array
            Array with Type frequency for all local environments for all types in TG. 
        """
        unqlbl = np.unique(self.Type)
        
        if sum(x is not None for x in [ordr, typevec, k]) != 1:
            raise ValueError('Exactly one of ordr, typevec, or k must be provided.')
        

        # arrange the indexes for the environments. 
        # if we use ordr this is neighborhood defined by iGraph
        # if we provide types, than indexes of each type. 
        if ordr is not None:
            # ind = self.SG.neighborhood(order = ordr)
            ind = tmgu.find_graph_neighborhoold_parallel_by_components(self.SG,ordr)
        elif typevec is not None:
            ind = list()
            for i in range(len(np.unique(typevec))):
                ind.append(np.flatnonzero(typevec==i))
        else:
            env_file_path = os.path.join(self.basepath, 'Layer', f"Env_{self.adata_mapping['Type'][:-3]}_{k}.npy")
            if os.path.exists(env_file_path) and weights is None:
                print("loading Env from file")
                return np.load(env_file_path)
            nbrs_ind,nbrs_dist = self.knn_query((self.XY,self.Section),k=k)
            typ = self.Type.astype(int)
            nbrs_types = typ[nbrs_ind.astype(int)]
            if N is None: 
                N = self.Type.max()+1
            Env = np.zeros((nbrs_types.shape[0], N), dtype=int)
            # Use np.add.at to accumulate counts directly into the occurrence matrix
            if weights is None: 
                np.add.at(Env, (np.arange(nbrs_types.shape[0])[:, None], nbrs_types), 1)
                np.save(env_file_path, Env)
            else:
                if k<5: 
                    raise ValueError("Weights not implemented for k<5")
                nbrs_types = typ[nbrs_ind.astype(int)]
                local_dens = nbrs_dist[:,5][:,None] * weights
                nbrs_weights = np.exp(-(nbrs_dist**2)/(local_dens**2)) 
                np.add.at(Env, (np.arange(nbrs_types.shape[0])[:, None], nbrs_types), nbrs_weights)
            
            return Env

        
        Env = np.zeros((len(ind),len(unqlbl)),dtype=np.int64)
        ndsz = self.node_size.copy().astype(np.int64)
        int_types = self.Type.astype(np.int64)
        AllTypes = [int_types[ix] for ix in ind]
        AllSizes = [ndsz[ix] for ix in ind]
        for i in range(Env.shape[0]):
            Env[i,:]=np.bincount(AllTypes[i],weights = AllSizes[i],minlength=len(unqlbl))
        
        # should be the same as above, but much slower... keeping it here for now till more testing is done. 
        # for i in range(len(ind)):
        #     Env[i,:]=count_values(self.Type[ind[i]],unqlbl,ndsz[ind[i]],norm_to_one = False)
        return(Env)
    
    def graph_local_avg(self,VecToSmooth,ordr = 3,kernel = np.array([0.4,0.3,0.2,0.1])):
        """Simple local average of a Vec based on local neighborgood
        
        Parameters
        ----------
            VecToSmooth : numpy array
                The values we want to smooth. len(VecToSmooth) must be self.N
        """
        
        if len(VecToSmooth) is not self.N: 
            raise ValueError(f"Length of input vector {len(VecToSmooth)} doesn't match TG.N which is {self.N}")
        
        Smoothed = np.zeros((len(VecToSmooth),ordr+1))
        Smoothed[:,0] = VecToSmooth
        for i in range(ordr):
            ind = self.SG.neighborhood(order = i+1,mindist=i)
            for j in range(len(ind)):
                ix = np.array(ind[j],dtype=np.int64)
                Smoothed[j,i+1] = np.nanmean(VecToSmooth[ix])

        kernel = kernel[None,:]
        kernel = np.repeat(kernel,len(VecToSmooth),axis=0)
        ix_nan = np.isnan(Smoothed)
        kernel[ix_nan]=0
        sum_of_rows = kernel.sum(axis=1)
        kernel = kernel / sum_of_rows[:, None]
        Smoothed[ix_nan] = 0
        Smoothed = np.multiply(Smoothed,kernel)
        Smoothed = Smoothed.sum(axis=1)
        return(Smoothed)
    
    def watershed(self,InputVec):
        """Watershed segmentation based on InputVec values
        
        Watershed on the TG spatial graph. 
        First finds local peaks and then assigns all nodes to their closest local peak using dijkstra
        
        Parameters
        ----------
        
        Return
        ------
        (id,dist) 
            tuple with id and distance to cloest zone. 
        """
        is_peak = np.zeros(InputVec.shape).astype('bool')
        ind = self.SG.neighborhood(order = 1,mindist=1)
        for i in range(len(ind)):
            is_peak[i] = np.all(InputVec[i]>InputVec[ind[i]])
        peaks = np.flatnonzero(is_peak)  

        Adj = self.SG.get_adjacency_sparse()
        Dij_min, predecessors, ClosestPeak = dijkstra(Adj, directed=False, 
                                                          indices=peaks, 
                                                          return_predecessors=True, 
                                                          unweighted=False, 
                                                          limit=np.inf, 
                                                          min_only=True)
        
        # renumber all closest peak continously
        u,ix_rev = np.unique(ClosestPeak, return_inverse=True)
        ClosestPeak=u[ix_rev]
        
        # relabel HoodId in case we have a heterozone that was split along the way
        # by contracting and expanding where each contracted zone gets a unique ID we
        # basically garantuee that Ntypes = N (for now...)
        CG = self.contract_graph(TypeVec = ClosestPeak)
        Id = np.arange(CG.N)
        ClosestPeak = Id[CG.Upstream]
        return (ClosestPeak, Dij_min)
        
    def calc_entropy_at_different_Leiden_resolutions(self,Rvec = np.logspace(-1,2.5,100)): 
        Ent = np.zeros(Rvec.shape)
        Ntypes = np.zeros(Rvec.shape)
        for i in range(len(Rvec)):
            TypeVec = self.FG["G"].community_leiden(resolution_parameter=Rvec[i],objective_function='modularity').membership
            self.update_user(f"iter: {i}")
            TypeVec = self.FG.community_leiden(resolution_parameter=Rvec[i],objective_function='modularity').membership
            TypeVec = np.array(TypeVec).astype(np.int64)
            Ent[i] = self.contract_graph(TypeVec,return_useful_layer = False).cond_entropy()
            Ntypes[i] = len(np.unique(TypeVec))
            
        self.cond_entropy_df = pd.DataFrame(data = {'Entropy' : Ent, 'Ntypes' : Ntypes, 'Resolution' : Rvec})  
    
    def gradient_magnitude(self,V):
        """Spatial gradient based on spatial graph
        
        Calculate the gradient defined as sqrt(dV/dx^2+dV/dy^2) where dV/dx(y) is calcualted using simple trigo
        
        """
        EL = self.SG.get_edgelist()
        EL = np.array(EL,dtype='int')
        XY = self.XY
        dV = V[EL[:,1]]-V[EL[:,0]]
        dX = XY[EL[:,1],0]-XY[EL[:,0],0]
        dY = XY[EL[:,1],1]-XY[EL[:,0],1]                      
        alpha = np.arctan(dX/dY)
        dVdX = dV*np.cos(alpha)/dX
        dVdY = dV*np.sin(alpha)/dY 
        dVdXY = np.sqrt(dVdX**2 + dVdY**2)
        df = pd.DataFrame({'dVdXY' : np.hstack((dVdXY,-dVdXY))})
        df['type']=np.hstack((EL[:,0],EL[:,1]))
        gradmag = np.array(df.groupby(['type']).mean())
        gradmag = np.array(gradmag)
        return(gradmag)
    
    def graph_local_median(self,VecToSmooth,ordr = 3):
        """local median of VecToSmooth based in spatial graph
        """
        ind = self.SG.neighborhood(order = ordr)
        Smoothed = np.zeros(VecToSmooth.shape)
        for j in range(len(ind)):
            ix = np.array(ind[j],dtype=np.int64)
            Smoothed[j] = np.nanmedian(VecToSmooth[ix])
        return(Smoothed)

class Taxonomy:
    """Taxonomical system for different biospatial units (cells, isozones, regions)
    
    Attributes
    ----------
    name : str
        a name of the taxonomy (will be used to file io)
    
    """
    
    def __init__(self, name=None, basepath = None,
        Types=None, feature_mat=None,rgb_codes=None,
        upstream_tax = None, upstream_types = None
        ): 
        """Create a Taxonomy object

        Parameters
        ----------
        name : str
            the name of the taxonomy object
        Types : list 
            list of types names that will be used in this Taxonomy
        """
        if name is None: 
            raise ValueError("Taxonomy must get a name")
        self.name = name
        self.basepath = basepath
        self.adata = anndata.AnnData(X=feature_mat)
        self.adata.obs["Type"] = Types

        # add RGB values if provided
        if rgb_codes is not None:
            self.adata.obs['RGB'] = list(rgb_codes)

        if upstream_tax is not None: 
            self.upstream_tax = upstream_tax
        
        if upstream_types is not None: 
            if upstream_tax is None: 
                raise ValueError("If you supply upstream types, you must supply upstream_tax as well")
            self.upstream_type = upstream_types

        return None
    
    @property
    def Type(self): 
        """list: Building blocks of this Taxonomy
        
        Setter verify that there are no duplications. 
        """
        return(np.array(self.adata.obs["Type"]))
    
    @Type.setter
    def Type(self,Types):
        if len(Types) is not len(set(Types)):
            dups = [Types.count(t) for t in set(Types) if Types.count(t)>1]
            raise ValueError(f"Types must be unique. Found duplicates: {dups}")
        if not (len(Types) == self.adata.shape[0]): 
            raise ValueError("Wrong number of types to assign - just remake the Tax")
        self.adata.obs['Type'] = Types

    @property
    def feature_mat(self): 
        """ndarray: feature values for all types in the taxonomy
        
        Setter can get as input either numpy array (ordered same as self.Type) or pandas dataframe
        """
        if self.is_empty(): 
            return None
        else: 
            return(self.adata.X)
        
    @feature_mat.setter
    def feature_mat(self,F):
        # first, make sure F is a DataFrame
        if self.adata.shape[0]==F.shape[0]: 
            self.adata.X=F
        else: 
            raise ValueError("Wrong number of rows")
        
    def is_empty(self):
        """ determie if taxonomy is empty
        """
        return (self.adata.shape[0] == 0)
        
    def save(self):
        """save to basepath using Taxonomy name
        """
        if not os.path.exists(self.basepath):
            os.makedirs(self.basepath)   
        if not self.is_empty():
            self.adata.write_h5ad(os.path.join(self.basepath, f"{self.name}.h5ad"))
                
    def load(self):
        """save from basepath using Taxonomy name
        """
        filename = os.path.join(self.basepath, f"{self.name}.h5ad")
        if os.path.exists(filename):
            self.adata = anndata.read_h5ad(filename)
        else: 
            warnings.warn(f"Taxonomy file {filename} not found")
    
    @property
    def upstream_tax(self): 
        if "upstream_tax" in self.adata.uns:
            return self.adata.uns["upstream_tax"]
        else:
            return None

    @upstream_tax.setter
    def upstream_tax(self,upstream_tax):
        self.adata.uns["upstream_tax"]=upstream_tax

    @property
    def upstream_type(self): 
        if "upstream_type" in self.adata.obs:
            return self.adata.obs["upstream_type"]
        else: 
            return None
        
    @upstream_type.setter
    def upstream_type(self,upstream_type):
        if "upstream_tax" not in self.adata.uns:
            raise ValueError("Set upstream_tax before adding a type")
        
        self.adata.obs["upstream_type"] = upstream_type

    @property
    def N(self):
        return len(self.Type)

    @property
    def RGB(self):
        """
        Returns RGB values for all types (random value if none assigned)
        """
        if 'RGB' in self.adata.obs.columns:
            rgb = list(self.adata.obs['RGB'])
            if isinstance(rgb[0],str): 
                # if RGB is string, convert to tuples of 0-255
                rgb = coloru.hex_to_rgb(rgb)
                rgb = np.array(rgb)
        else: 
            rgb = coloru.rand_hex_codes(self.adata.shape[0])
        return rgb

    @RGB.setter
    def RGB(self,rgb_codes):
        self.adata.obs['RGB'] = rgb_codes

    def get_type_ix(self,typ_str):
        Type_ix_dict = {item: i for i, item in enumerate(self.Type)}
        index_positions = [Type_ix_dict.get(item) for item in typ_str]
        if any(pos is None for pos in index_positions):
            raise ValueError("One or more specified types are not found in the Type dictionary.")
        return index_positions

class Geom:
    """
    Container class for a collection of polygons. 
    These geometires could be used to represent the units in a TissueGraphs but other uses are possible. 
    
    Supported Geoms: 
        * voronoi
        * cells
        * nuclei 
        * cytoplasm 
        * isozones 
        * regions 

        NOTE (1/25/2023): apperantly, extracting coordinates from 100K polygons in shapely is pretty slow
        therefore, during construction, doing this calculation once and caching it as _vert. 
        A wrap-around property, vert, serves that. If this could be faster, might make sense to calculate that on the fly. 

        For the vertices representation. Verts is a tuple of (ext,int) one for each polygon. Ext has one polygon and int a list of 
        potentially many "holes". The verts are computed by get_polygons_vertices and by keeping them in memory we are avoiding
        recomputing this every time the geomtry is used (mostly for plotting). This does add one-time cost during 
        __init__ to create this cached precomputed representation. 

        NOTE (6/8/2024): enhancing Geom to allow representation of multipolygons as basic geom shape. 
        This mostly means that verts 

    """

    def __init__(self,geom_type = None,polys = None,basepath = None,section = None, redo = False):
        self.type = geom_type
        self.geom_type = geom_type
        self.section = section
        self.basepath = os.path.join(basepath,'Geom',section)
        self.filename = os.path.join(self.basepath, f"{self.geom_type}.wkt")
        # read from drive if exists already
        if polys is None and os.path.exists(self.filename): 
            polys = fileu.load_polygon_list(self.filename)
        self.polys = polys
        self._verts = None
        


    @property
    def verts(self):
        # convert poly to verts on first use
        if self._verts is None: 
            self._verts = geomu.get_polygons_vertices(self.polys)
        return self._verts

    def save(self):
        if not os.path.exists(self.basepath):
            os.makedirs(self.basepath)
        fileu.save_polygon_list(self.polys,self.filename)

       
    def get_xylim(self):
        """Returns the min/max xy of the geometry
        """
        # get xy of exterior coordinates
        xy = np.vstack(self.verts[0])
        # get limits
        xlim = [xy[:,0].min(), xy[:,0].max()]
        ylim = [xy[:,1].min(), xy[:,1].max()]
        return xlim,ylim
        

    